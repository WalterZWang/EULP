{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy\n",
    "import itertools\n",
    "import warnings\n",
    "import math\n",
    "from pvlib.iotools import read_epw\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import calplot\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "\n",
    "sns.set()\n",
    "\n",
    "n_clusters = 27\n",
    "\n",
    "start_folder_cluster     = '/Users/jgonzal2/EULP/result/clustering'\n",
    "start_folder_time_domain = '/Users/jgonzal2/EULP/result/combined_stats'\n",
    "start_folder_sqft        = '/Users/jgonzal2/EULP/data_files/data/energy_intensity'\n",
    "start_folder_cs_sqft     = '/Users/jgonzal2/EULP/data_files/data/comstock_energy_intensity'\n",
    "# type_folders = ['by_utility', 'by_building_type', 'by_utility_and_building_type']\n",
    "type_folders = ['by_building_type', 'by_utility_and_building_type']\n",
    "\n",
    "cluster_cols = [f'{k+2}_clusters' for k in range(1,n_clusters+1)]\n",
    "hourly_cols  = [f'hour_{str(i).rjust(2, \"0\")}' for i in range(24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_v(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width, im1.height + im2.height), (255, 255, 255))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (0, im1.height))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d158b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_season(val):\n",
    "    if val<55:\n",
    "        return 'winter'\n",
    "    elif val>=55 and val<70:\n",
    "        return 'shoulder'\n",
    "    else:\n",
    "        return 'summer'\n",
    "\n",
    "def c_to_f(c):\n",
    "    return (c * 9.0/5.0) + 32.0\n",
    "\n",
    "def is_weekend(val):\n",
    "    if val in ['Monday','Tuesday', 'Wednesday', 'Thursday' ,'Friday']:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "weather_data_daily_dict   = {}\n",
    "weather_data_hourly_dict  = {}\n",
    "weather_metadata_dict     = {}\n",
    "file_overview = pd.read_csv('/Users/jgonzal2/EULP/data_files/weather_files/weather_file_list_for_ami_analysis.csv')\n",
    "\n",
    "all_weather_data_daily = pd.DataFrame(columns=['temp_air', 'date_no_time', 'timestamp'])\n",
    "all_weather_data_hourly = pd.DataFrame(columns=['temp_air', 'timestamp'])\n",
    "\n",
    "for ix, row in file_overview.iterrows():\n",
    "    year    = row['year']\n",
    "    utility = row['utility_name']\n",
    "    fips    = row['match_FIP']\n",
    "    \n",
    "    data, metadata = read_epw(f'/Users/jgonzal2/EULP/data_files/weather_files/{year}_{fips}.epw')\n",
    "    \n",
    "    data              = data[['temp_air']]\n",
    "    data['date']      = data.index.date\n",
    "    \n",
    "    data_hourly = data.copy()\n",
    "    data_hourly['timestamp'] = pd.to_datetime(data_hourly.index, utc=True, format='%Y-%m-%d %H:%M:%S')\n",
    "    data_hourly['utility'] = utility\n",
    "    \n",
    "    data              = data.groupby('date').mean()\n",
    "    \n",
    "    data['utility'] = utility\n",
    "    data['date_no_time'] = pd.to_datetime(data.index, utc=True, format='%Y-%m-%d').date\n",
    "    \n",
    "    data['season'] = data['temp_air'].apply(c_to_f).apply(define_season)\n",
    "    \n",
    "    if ix == 0:\n",
    "        all_weather_data_daily = data.copy()\n",
    "        \n",
    "        all_weather_data_hourly = data_hourly.copy()\n",
    "        \n",
    "    else:\n",
    "        all_weather_data_daily = all_weather_data_daily.append(data)\n",
    "        all_weather_data_hourly = all_weather_data_hourly.append(data_hourly)\n",
    "    \n",
    "    weather_data_daily_dict[f'{year}_{utility}']  = data\n",
    "    weather_data_hourly_dict[f'{year}_{utility}'] = data_hourly\n",
    "    weather_metadata_dict[f'{year}_{utility}']    = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03be3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(P,Q):\n",
    "    \"\"\" Epsilon is used here to avoid conditional code for\n",
    "    checking that neither P nor Q is equal to 0. \"\"\"\n",
    "    epsilon = 0.0000001\n",
    "\n",
    "     # You may want to instead make copies to avoid changing the np arrays.\n",
    "    P_copy = P.copy().fillna(epsilon)\n",
    "    Q_copy = Q.copy().fillna(epsilon)\n",
    "    \n",
    "    #normalize P, Q\n",
    "    P_copy = P_copy/P_copy.sum()\n",
    "    Q_copy = Q_copy/Q_copy.sum()\n",
    "\n",
    "    divergence = np.sum(P_copy*np.log(P_copy/Q_copy))\n",
    "    return divergence\n",
    "\n",
    "def myround(x, base=None):\n",
    "    return base * round(x/base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09050a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGraphs:\n",
    "    \n",
    "    def __init__(self, start_folder, folder, subfolder):\n",
    "        self.start_folder   = start_folder\n",
    "        self.folder         = folder\n",
    "        self.subfolder      = subfolder\n",
    "        \n",
    "        self.cluster_result = pd.read_csv(f'{self.start_folder}/{self.folder}/{self.subfolder}/cluster_result.csv', parse_dates=['date'])\n",
    "        self.cluster_result = self.cluster_result[self.cluster_result['bad_data']==False]\n",
    "        self.cluster_result = self.cluster_result.rename(columns = {'data_type':'utility'})\n",
    "        self.cluster_result['date_no_time'] = pd.to_datetime(self.cluster_result['date'], utc=True, format='%Y-%m-%d').dt.date\n",
    "        self.cluster_result['weekend'] = self.cluster_result['day_of_week'].apply(is_weekend)\n",
    "        \n",
    "        self.cluster_data   = self.cluster_result[cluster_cols]\n",
    "        self.hourly_data    = self.cluster_result[hourly_cols]\n",
    "        \n",
    "        self.n_clusters     = 27\n",
    "        self.clusters       = self.cluster_data['29_clusters'].unique()\n",
    "        self.calc_clusters  = len(self.clusters)\n",
    "        \n",
    "        self.maxval_1 = None\n",
    "        self.maxval_2 = None\n",
    "        self.maxval_3 = None\n",
    "        \n",
    "        # merge cluster results with weather files\n",
    "        self.cluster_result = pd.merge(self.cluster_result, all_weather_data_daily, on = ['date_no_time', 'utility'], how='left').drop('date_no_time', axis=1)\n",
    "        \n",
    "    def get_maxval_1(self, data):\n",
    "        if self.maxval_1 is None:\n",
    "            \n",
    "            self.maxval_1 = data.groupby('cluster_group').mean().max().max()\n",
    "\n",
    "            if self.maxval_1 <= 1:\n",
    "                return 1.05\n",
    "            return 1.5\n",
    "        \n",
    "        return self.maxval_1\n",
    "    \n",
    "    \n",
    "    # sort the values in the pivot_freq[col] series by frequency in descending order\n",
    "    # keep all values whose sum does not exceed the percentile\n",
    "    # include the last value that makes the sum >= the percentile\n",
    "            \n",
    "    # example, percentile = 0.90\n",
    "    # values   = [0.4, 0.05, 0.2, 0.1, 0.2, 0.05]\n",
    "    # selected = [0.4, nan, 0.2, 0.1, 0.2, nan]\n",
    "    def get_cluster_percentile(self, pivot, percentile):\n",
    "        pivot      = pivot.copy()\n",
    "        pivot_freq = pivot.copy()\n",
    "        \n",
    "        for col in pivot_freq.columns:\n",
    "            pivot_freq[col] = pivot_freq[col]/pivot_freq[col].sum()\n",
    "            \n",
    "            temp_sum = 0\n",
    "            temp_series = pivot_freq[col].sort_values()\n",
    "            temp_series = temp_series[::-1]\n",
    "            \n",
    "            for value in temp_series:\n",
    "                \n",
    "                if pd.isna(value):\n",
    "                    continue\n",
    "                temp_sum += value\n",
    "                \n",
    "                if temp_sum >= percentile:\n",
    "                    smallest_value = value\n",
    "                    break\n",
    "            \n",
    "            pivot_freq[col] = pivot_freq[col].apply(lambda x: x if x >= smallest_value  else np.nan)\n",
    "            mask = pivot_freq[col].isna()\n",
    "            \n",
    "            temp = pivot[col]\n",
    "            temp[mask] = np.nan\n",
    "            \n",
    "            pivot[col] = temp\n",
    "            \n",
    "        return pivot\n",
    "        \n",
    "        \n",
    "    # Plot the average load profiles for each cluster group for each building type, varying k\n",
    "    def plot_graph_1(self, show):\n",
    "            \n",
    "            ## GRAPH 1\n",
    "            # plot mean load shape for k-means range from k=2 to k=n_clusters\n",
    "            fig, axes = plt.subplots(nrows=self.n_clusters-1, ncols=1, figsize=(8,75))\n",
    "\n",
    "            for k in range(1,self.n_clusters):\n",
    "                data_to_plot = self.hourly_data.copy()\n",
    "                data_to_plot['cluster_group'] = self.cluster_data[f'{k+2}_clusters'].copy()\n",
    "                \n",
    "                if k == 1:\n",
    "                    self.maxval_1 = self.get_maxval_1(data_to_plot)\n",
    "\n",
    "                data_to_plot.groupby('cluster_group').mean().T.plot(ax=axes[k-1],legend=None)\n",
    "\n",
    "                axes[k-1].set_ylim([-.03, self.maxval_1*1.05])\n",
    "                axes[k-1].set_xlim([0,23])\n",
    "\n",
    "                axes[k-1].set_ylabel(f'k = {k+1}, load / load_99')\n",
    "\n",
    "                plt.suptitle(f'Clustering results for {self.subfolder} load profiles, n={len(self.cluster_result)} profiles')\n",
    "\n",
    "            handles, labels = axes[-1].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, loc='lower center', ncol = int((self.n_clusters+3)/4), title=\"Cluster Groups\")#, mode = \"expand\")\n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(top=0.975)\n",
    "            fig.subplots_adjust(bottom=0.03)\n",
    "\n",
    "            plt.savefig(f'{self.start_folder}/{self.folder}/{self.subfolder}/load_profile_cluster_groups.png')\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close('all')\n",
    "            \n",
    "            \n",
    "    # Plot the load profiles for each cluster group for each building type    \n",
    "    def plot_graph_2(self, show, one_col=False):\n",
    "\n",
    "            data_to_plot = self.hourly_data.copy()\n",
    "            data_to_plot['cluster_group'] = self.cluster_data[f'{self.n_clusters+2}_clusters'].copy()\n",
    "\n",
    "            self.maxval_1 = self.get_maxval_1(data_to_plot)\n",
    "\n",
    "            mean_data_to_plot = data_to_plot.groupby('cluster_group').mean().T\n",
    "            \n",
    "            if one_col:\n",
    "                fig, axes = plt.subplots(nrows=len(mean_data_to_plot.T), ncols=1, figsize=(3,75))\n",
    "            else:\n",
    "                fig, axes = plt.subplots(nrows=math.ceil(len(mean_data_to_plot.T)/6.0), ncols=math.ceil(len(mean_data_to_plot.T)/5.0), figsize=(30,20))\n",
    "            \n",
    "            cluster_labels = dict(zip(range(len(mean_data_to_plot.columns)), mean_data_to_plot.columns))\n",
    "            \n",
    "            for k, ax in enumerate(axes.reshape(-1)):\n",
    "                if k >= len(mean_data_to_plot.columns):\n",
    "                    continue\n",
    "\n",
    "                mean_data_to_plot[[cluster_labels[k]]].plot(ax=ax, legend=None)\n",
    "\n",
    "                frequency = 100.0 * len(data_to_plot[data_to_plot['cluster_group']==cluster_labels[k]]) / len(data_to_plot) \n",
    "\n",
    "                n = len(data_to_plot[data_to_plot['cluster_group']==cluster_labels[k]])\n",
    "\n",
    "                ax.set_title(f'cluster {cluster_labels[k]}, frequency = {frequency:.2f}%, n = {n}')\n",
    "                ax.set_ylabel('load / load_99')\n",
    "                \n",
    "                if one_col:\n",
    "                    ax.set_ylim([0, 1])\n",
    "                else:\n",
    "                    ax.set_ylim([-0.03, self.maxval_1*1.05])\n",
    "                    \n",
    "                ax.set_xlim([0, 23])\n",
    "\n",
    "            plt.suptitle(f'Load profile overview for {self.subfolder}, n = {len(data_to_plot)} profiles')\n",
    "\n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(top=0.96, bottom=0.03)\n",
    "            \n",
    "            plt.savefig(f'{self.start_folder}/{self.folder}/{self.subfolder}/load_profile_frequency.png')\n",
    "            \n",
    "            \n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close('all')\n",
    "\n",
    "\n",
    "    # Plot the calendar heat map for each building type\n",
    "    # All years are plotted\n",
    "    def plot_graph_3(self, show):\n",
    "\n",
    "            to_pivot = self.cluster_result[['date', f'{n_clusters+2}_clusters']]\n",
    "            pivot = pd.pivot_table(to_pivot, index='date', columns=f'{self.n_clusters+2}_clusters', aggfunc=len)\n",
    "            pivot.index = pd.to_datetime(pivot.index, utc=True)\n",
    "\n",
    "            data_to_plot = self.hourly_data.copy()\n",
    "            data_to_plot['cluster_group'] = self.cluster_data[f'{n_clusters+2}_clusters']\n",
    "\n",
    "            self.maxval_1 = self.get_maxval_1(data_to_plot)\n",
    "\n",
    "            for year in list(pivot.index.year.unique()):\n",
    "\n",
    "                pivot_to_plot = pivot[pivot.index.year == year]\n",
    "                \n",
    "                self.maxval_2 = pivot_to_plot.max().max()\n",
    "\n",
    "                for k in pivot_to_plot.columns:\n",
    "                    # plot curve\n",
    "                    fig = plt.figure(constrained_layout = True, figsize = (21,3))\n",
    "                    gridspec = fig.add_gridspec(nrows=1, ncols = 7)\n",
    "\n",
    "                    fig_ax_0 = fig.add_subplot(gridspec[0, 0])\n",
    "                    fig_ax_0.set_ylabel(\"load / load_99\")\n",
    "\n",
    "                    fig_ax_1 = fig.add_subplot(gridspec[0, 1])\n",
    "                    fig_ax_1.set_ylabel(\"load / load_99\")\n",
    "\n",
    "                    sns.set()\n",
    "\n",
    "                    ## GRAPH 3A\n",
    "                    # plot all the cluster curves on fig_ax_0\n",
    "                    # randomly select up to 300 curves to plot for performance purposes\n",
    "                    data_to_plot[data_to_plot['cluster_group']==k].sample(min(len(data_to_plot[data_to_plot['cluster_group']==k]), 300)).T.plot(ax=fig_ax_0, alpha=0.1, legend=None, color='b', label='individual profiles')\n",
    "\n",
    "                    # plot the mean cluster curve on fig_ax_0\n",
    "                    data_to_plot[data_to_plot['cluster_group']==k].groupby('cluster_group').mean().T.plot(ax=fig_ax_0,legend=None, linewidth=1.25, color='k', label=f'{year} mean')\n",
    "                    \n",
    "                    fig_ax_0.set_ylim([-.03, self.maxval_1*1.05])\n",
    "                    fig_ax_0.set_xlim([0,23])\n",
    "\n",
    "                    # plot the standard deviation curve on fig_ax_1\n",
    "                    sns.boxplot(data = data_to_plot[data_to_plot['cluster_group']==k].drop('cluster_group', axis=1), showfliers=False, color='b')\n",
    "                    fig_ax_1.set_xticks([0,5,10,15,20])\n",
    "                    fig_ax_1.set_ylim([-.03, self.maxval_1*1.05])\n",
    "                    fig_ax_1.set_xlim([0,23])\n",
    "\n",
    "                    ## GRAPH 3B\n",
    "                    # plot heatmap\n",
    "                    fig_ax_2 = fig.add_subplot(gridspec[0, 2:])\n",
    "\n",
    "                    cax = calplot.yearplot(pivot_to_plot[pivot_to_plot[k].notnull()][k].resample('D').sum(),\n",
    "                                    cmap=u'Reds',\n",
    "                                    how=u'sum',\n",
    "                                    year=year,\n",
    "                                    fillcolor=u'whitesmoke',\n",
    "                                    linewidth=1, \n",
    "                                    linecolor=None, \n",
    "                                    vmin = 0,\n",
    "                                    vmax = self.maxval_2,\n",
    "                                    daylabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], \n",
    "                                    dayticks=True, \n",
    "                                    monthlabels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "                                    monthticks=True, \n",
    "                                    ax=fig_ax_2)\n",
    "\n",
    "                    divider = make_axes_locatable(cax)\n",
    "                    lcax = divider.append_axes(\"right\", size=\"2%\", pad=0.75)\n",
    "                    fig.colorbar(cax.get_children()[1], cax=lcax)\n",
    "\n",
    "                    frequency = 100.0 * len(data_to_plot[data_to_plot['cluster_group']==k]) / len(data_to_plot)\n",
    "                    n = int(pivot_to_plot[k].sum())\n",
    "\n",
    "                    plt.suptitle(f'Load profile overview for {self.subfolder} cluster {k}, {year}, frequency = {frequency:.2f}%, n = {n} profiles')\n",
    "                    if not os.path.exists(f'{self.start_folder}/{self.folder}/{self.subfolder}/heat_maps'):\n",
    "                        os.makedirs(f'{self.start_folder}/{self.folder}/{self.subfolder}/heat_maps')\n",
    "                    \n",
    "                    plt.savefig(f'{self.start_folder}/{self.folder}/{self.subfolder}/heat_maps/load_profile_heatmap_{year}_cluster_{k}.png', bbox_inches=\"tight\")\n",
    "\n",
    "                    if show:\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        plt.close('all')\n",
    "\n",
    "                    print(f'{subfolder}, {year}, {k}')\n",
    "\n",
    "    \n",
    "    # Plot the count of each cluster group for each building type/utility\n",
    "    # Use this plot to show the similarity between utilities\n",
    "    # Show the KL divergense and JS distance\n",
    "    def plot_graph_4(self, show, divergence_method):\n",
    "        if folder != 'by_building_type':\n",
    "            return\n",
    "\n",
    "        cluster_cols = ['5_clusters', '10_clusters', '15_clusters', '20_clusters', '25_clusters', '29_clusters']\n",
    "\n",
    "        fig = plt.figure(figsize=(100,13)) \n",
    "        outer_grid = gridspec.GridSpec(nrows = 2, ncols = 1, figure = fig)\n",
    "\n",
    "        upper_cell = outer_grid[0,0]\n",
    "        lower_cell = outer_grid[1,0]\n",
    "\n",
    "        upper_grid = gridspec.GridSpecFromSubplotSpec(1, len(cluster_cols), upper_cell, width_ratios=[8,9,14,19,24,28])\n",
    "        lower_grid = gridspec.GridSpecFromSubplotSpec(1, len(cluster_cols), lower_cell, width_ratios=[8,9,14,19,24,28])\n",
    "\n",
    "        for i, cluster_col in enumerate(cluster_cols):\n",
    "            ax_0 = plt.subplot(upper_grid[0, i])\n",
    "            ax_1 = plt.subplot(lower_grid[0, i])\n",
    "\n",
    "            to_pivot = self.cluster_result[[cluster_col,'utility']]\n",
    "            # exclude fort collins from this analysis since there is very little fort collins data \n",
    "            to_pivot = to_pivot[to_pivot['utility']!='fortcollins']\n",
    "            \n",
    "            pivot = to_pivot.pivot_table(index=cluster_col, columns='utility', aggfunc=len)\n",
    "\n",
    "            # normalize the cluster frequencies\n",
    "            for col in pivot.columns:\n",
    "                pivot[col] = pivot[col] /  pivot[col].sum()\n",
    "\n",
    "            if i==0:\n",
    "                self.maxval_3 = pivot.max().max()\n",
    "\n",
    "            # create the bar plots\n",
    "            pivot['cluster #'] = pivot.index\n",
    "            pivot.plot(x='cluster #', kind = 'bar', stacked = False, title=f'{self.subfolder} Cluster Frequency by Utility Group, k = {len(pivot)}', ax=ax_0)\n",
    "\n",
    "            ax_0.set_ylim([0,self.maxval_3 *1.05])\n",
    "\n",
    "            # create the stair plot showing correlation between all utilities\n",
    "            dat = self.cluster_result[self.cluster_result['bad_data']==False][[cluster_col,'utility']]\n",
    "            # exclude fort collins from this analysis since there is very little fort collins data \n",
    "            dat = dat[dat['utility']!='fortcollins']\n",
    "            dat = dat.pivot_table(index=cluster_col, columns='utility', aggfunc=len)\n",
    "\n",
    "            for col in dat.columns:\n",
    "                dat[col] = dat[col] / dat[col].sum()\n",
    "            \n",
    "            # create dummy correlation matrix\n",
    "            div_matrix = dat.fillna(0).corr()\n",
    "\n",
    "            # replace the data with KL divervence coefficients\n",
    "            for col in div_matrix.columns:\n",
    "                for row in div_matrix.index:\n",
    "                    if row==col:\n",
    "                        continue\n",
    "                    \n",
    "                    if divergence_method == 'js':\n",
    "                        div_matrix.loc[row,col] = scipy.spatial.distance.jensenshannon(dat[row].fillna(0), dat[col].fillna(0))\n",
    "                    elif divergence_method == 'kl':\n",
    "                        div_matrix.loc[row,col] = kl_div(dat[row], dat[col])\n",
    "\n",
    "                    \n",
    "            mask = np.zeros_like(div_matrix, dtype=bool)\n",
    "            mask[np.triu_indices_from(mask)] = True\n",
    "            div_matrix[mask] = np.nan\n",
    "\n",
    "            cmap = sns.diverging_palette(133,10, as_cmap=True)\n",
    "\n",
    "            sns.axes_style(\"white\")\n",
    "            g= sns.heatmap(div_matrix, mask=mask, ax = ax_1, vmin = 0, vmax = 1, square=True, annot = True, cmap = cmap)\n",
    "            g.set_facecolor('white')\n",
    "\n",
    "            ax_1.set_aspect('equal')\n",
    "            \n",
    "            divergence_method_map = {'js': 'Jensen-Shannon Distance','kl': 'Kullbackâ€“Leibler Divergence'}\n",
    "\n",
    "            ax_1.set_title(f'{self.subfolder} {divergence_method_map[divergence_method]} Between Utilities, k = {len(pivot)}')\n",
    "\n",
    "        plt.savefig(f'{self.start_folder}/{self.folder}/{self.subfolder}/cluster_comparision_{divergence_method}.png')\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close('all')\n",
    "            \n",
    "    \n",
    "    # Plot the number of unique clusters per building per building type\n",
    "    # A building with a high number of clusters indicates high variability\n",
    "    # A building with a low number of clusters indicates low variability\n",
    "    def plot_graph_5(self, show, percentile=None):\n",
    "        if self.folder != 'by_building_type':\n",
    "            return\n",
    "        \n",
    "        to_pivot = self.cluster_result[['29_clusters','building_ID', 'utility']]\n",
    "        pivot = to_pivot.pivot_table(index='29_clusters', columns=['utility', 'building_ID'], aggfunc=len)\n",
    "        \n",
    "        if percentile is not None:\n",
    "            pivot = self.get_cluster_percentile(pivot, percentile)\n",
    "                \n",
    "        to_histogram = pd.Series(data=[0 for i in range(len(pivot.columns))], index = pivot.columns)\n",
    "        \n",
    "        for col in pivot.columns:\n",
    "            to_histogram[col] = len(pivot[pivot[col].notnull()][col])\n",
    "        \n",
    "        n_utilities = len(to_pivot['utility'].unique())\n",
    "        \n",
    "        if n_utilities == 1:\n",
    "            ncols = 1\n",
    "        else:\n",
    "            ncols = 1 + n_utilities\n",
    "        \n",
    "        fig, ax = plt.subplots(nrows = 1, ncols = ncols, figsize = (7*ncols, 6))\n",
    "        \n",
    "        if ncols == 1:\n",
    "            ax_0 = ax\n",
    "        else:\n",
    "            ax_0 = ax[0]\n",
    "        \n",
    "        ax_0.set_xlim([0,30])\n",
    "        ax_0.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        if percentile is not None:\n",
    "            ax_0.set_title(f'{self.subfolder} cluster frequency per building\\n n = {len(to_histogram)} buildings, {n_utilities} utilities')\n",
    "            ax_0.set_title(f'{self.subfolder} cluster frequency per building\\n up to {int(percentile*100)}th percentile, n = {len(to_histogram)} buildings \\n{n_utilities} utilities')\n",
    "            \n",
    "        else:\n",
    "            ax_0.set_title(f'{self.subfolder} cluster frequency per building\\n n = {len(to_histogram)} buildings, {n_utilities} utilities')\n",
    "        ax_0.set_xlabel(f'number of unique clusters for one building')\n",
    "        ax_0.set_ylabel(f'frequency')\n",
    "        \n",
    "        sns.histplot(to_histogram, ax=ax_0, discrete=True)\n",
    "        \n",
    "        y_lim = ax_0.get_ylim()\n",
    "        \n",
    "        palette = itertools.cycle(sns.color_palette())\n",
    "        next(palette)\n",
    "        color_map = {}\n",
    "        for utility in ['tallahassee', 'seattle', 'epb', 'veic', 'horry', 'cherryland', 'maine', 'fortcollins']:\n",
    "            color_map[utility] = next(palette)\n",
    "        \n",
    "        if ncols > 1:\n",
    "            for i, utility in enumerate(sorted(to_pivot['utility'].unique())):\n",
    "                temp_pivot = pivot[utility]\n",
    "                \n",
    "                to_histogram = pd.Series(data=[0 for i in range(len(temp_pivot.columns))], index = temp_pivot.columns)\n",
    "                \n",
    "                for col in temp_pivot.columns:\n",
    "                    to_histogram[col] = len(temp_pivot[temp_pivot[col].notnull()][col])\n",
    "\n",
    "                ax[i+1].set_xlim([0,30])\n",
    "                ax[i+1].set_ylim(y_lim)\n",
    "                ax[i+1].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                \n",
    "                if percentile is not None:\n",
    "                    ax[i+1].set_title(f'{utility} cluster frequency per building\\nup to {int(percentile*100)}th percentile, n = {len(to_histogram)} buildings')\n",
    "                    ax[i+1].set_xlabel(f'number of unique clusters for one building\\n up to {int(percentile*100)}th percentile')\n",
    "                \n",
    "                else:\n",
    "                    ax[i+1].set_title(f'{utility} cluster frequency per building\\nn = {len(to_histogram)} buildings')\n",
    "                    ax[i+1].set_xlabel(f'number of unique clusters for one building')\n",
    "                \n",
    "                ax[i+1].set_ylabel(f'frequency')\n",
    "                \n",
    "                c = next(palette)\n",
    "                \n",
    "                sns.histplot(to_histogram, ax=ax[i+1], discrete=True, color=color_map[utility])\n",
    "        if percentile is not None:\n",
    "            plt.savefig(f'{self.start_folder}/{self.folder}/{self.subfolder}/unique_cluster_count_per_building_distribution_{int(percentile*100)}_percentile.png')\n",
    "        \n",
    "        else:\n",
    "            plt.savefig(f'{self.start_folder}/{self.folder}/{self.subfolder}/unique_cluster_count_per_building_distribution.png')\n",
    "        \n",
    "        if show: \n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "    \n",
    "    \n",
    "    # Merge clustering data with weather data\n",
    "    # Show cluster % for each day type (weekday, weekend, holiday, summer, winter, shoulder...)\n",
    "    def plot_graph_6(self, show, normalize_by='row'):\n",
    "        \n",
    "        if self.folder != 'by_building_type':\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(f'{self.start_folder}/{self.folder}/{self.subfolder}/day_type_cluster_frequency'):\n",
    "            os.makedirs(f'{self.start_folder}/{self.folder}/{self.subfolder}/day_type_cluster_frequency')\n",
    "        \n",
    "        cluster_result_copy = self.cluster_result[['29_clusters', 'utility','season','weekend','holiday','building_ID']]\n",
    "        cluster_result_copy = cluster_result_copy.rename({'29_clusters':'cluster_number'}, axis=1)\n",
    "        \n",
    "        n_buildings = len(cluster_result_copy['building_ID'].unique())\n",
    "        n_utilities = len(cluster_result_copy['utility'].unique())\n",
    "        \n",
    "        criteria = [cluster_result_copy['season']   == 'winter',\n",
    "                    (cluster_result_copy['season']  == 'winter') & (cluster_result_copy['weekend']  == False),\n",
    "                    (cluster_result_copy['season']  == 'winter') & (cluster_result_copy['weekend']  == True),\n",
    "                    (cluster_result_copy['season']  == 'winter') & (cluster_result_copy['holiday']  == True),\n",
    "                    \n",
    "                    cluster_result_copy['season']   == 'shoulder',\n",
    "                    (cluster_result_copy['season']  == 'shoulder') & (cluster_result_copy['weekend']  == False),\n",
    "                    (cluster_result_copy['season']  == 'shoulder') & (cluster_result_copy['weekend']  == True),\n",
    "                    (cluster_result_copy['season']  == 'shoulder') & (cluster_result_copy['holiday']  == True),\n",
    "                    \n",
    "                    cluster_result_copy['season']   == 'summer',\n",
    "                    (cluster_result_copy['season']  == 'summer') & (cluster_result_copy['weekend']  == False),\n",
    "                    (cluster_result_copy['season']  == 'summer') & (cluster_result_copy['weekend']  == True),\n",
    "                    (cluster_result_copy['season']  == 'summer') & (cluster_result_copy['holiday']  == True),\n",
    "                    \n",
    "                    cluster_result_copy['weekend']  == False,\n",
    "                    cluster_result_copy['weekend']  == True,\n",
    "                    cluster_result_copy['holiday']  == True]\n",
    "        \n",
    "        descriptions = ['winter_day',   'winter_weekday',   'winter_weekend',   'winter_holiday',\n",
    "                        'shoulder_day', 'shoulder_weekday', 'shoulder_weekend', 'shoulder_holiday',\n",
    "                        'summer_day',   'summer_weekday',   'summer_weekend',   'summer_holiday',\n",
    "                        'weekday', 'weekend_day', 'holiday']\n",
    "        \n",
    "        to_plot, dominant_cluster = self._plot_graph_6_helper_1(criteria, descriptions, cluster_result_copy, normalize_by)\n",
    "        \n",
    "        if n_utilities > 1:\n",
    "            \n",
    "            title   = f'{self.subfolder} cluster frequency, n_buildings = {n_buildings}, n_utilities = {n_utilities}, normalized by {normalize_by}'\n",
    "            save_as = f'{self.start_folder}/{self.folder}/{self.subfolder}/day_type_cluster_frequency/day_type_cluster_frequency_all_buildings_norm_by_{normalize_by}'\n",
    "            self._plot_graph_6_helper_2(to_plot, dominant_cluster, title, save_as, show, normalize_by)\n",
    "            \n",
    "            for i, utility in enumerate(cluster_result_copy['utility'].unique()):\n",
    "                \n",
    "                # make plot for each utility\n",
    "                to_plot, dominant_cluster = self._plot_graph_6_helper_1(criteria, descriptions, cluster_result_copy[cluster_result_copy['utility']==utility], normalize_by)\n",
    "                \n",
    "                n_buildings = len(cluster_result_copy[cluster_result_copy['utility']==utility]['building_ID'].unique())\n",
    "                \n",
    "                title   = f'{utility} {self.subfolder} cluster frequency, n_buildings = {n_buildings}, normalized by {normalize_by}'\n",
    "                save_as = f'{self.start_folder}/{self.folder}/{self.subfolder}/day_type_cluster_frequency/day_type_cluster_frequency_{utility}_norm_by_{normalize_by}'\n",
    "                \n",
    "                self._plot_graph_6_helper_2(to_plot, dominant_cluster, title, save_as, show, normalize_by)\n",
    "\n",
    "            \n",
    "        else: \n",
    "            utility = cluster_result_copy['utility'].unique()[0]\n",
    "            \n",
    "            title   = f'{self.subfolder} cluster frequency, n_buildings = {n_buildings}, n_utilities = {n_utilities}, normalized by {normalize_by}'\n",
    "            save_as = f'{self.start_folder}/{self.folder}/{self.subfolder}/day_type_cluster_frequency/day_type_cluster_frequency_all_buildings_norm_by_{normalize_by}'\n",
    "            \n",
    "            self._plot_graph_6_helper_2(to_plot, dominant_cluster, title, save_as, show, normalize_by)\n",
    "            \n",
    "            \n",
    "    def _plot_graph_6_helper_1(self, criteria, descriptions, cluster_result_copy, normalize_by):\n",
    "        for i, criterion in enumerate(criteria):\n",
    "            row = cluster_result_copy[criterion].groupby('cluster_number').count()[['utility']]\n",
    "            \n",
    "            if normalize_by == 'row':\n",
    "                row['utility'] = row['utility']/(row['utility'].sum())\n",
    "            \n",
    "            row = row.rename({'utility':descriptions[i]}, axis=1)\n",
    "        \n",
    "            if i==0:\n",
    "                to_plot = row.T\n",
    "            else:\n",
    "                to_plot = to_plot.append(row.T)\n",
    "\n",
    "        if normalize_by == 'all':\n",
    "            to_plot = to_plot/(to_plot.sum().sum())\n",
    "               \n",
    "        dominant_cluster = pd.DataFrame(to_plot.idxmax(axis=1), columns=['dominant_cluster'])\n",
    "        \n",
    "        return to_plot, dominant_cluster\n",
    "        \n",
    "        \n",
    "    def _plot_graph_6_helper_2(self, to_plot, dominant_cluster, title, save_as, show, normalize_by):\n",
    "        fig = plt.figure(constrained_layout = True, figsize=(26,10))\n",
    "        gridspec = fig.add_gridspec(nrows=15, ncols = 20)\n",
    "            \n",
    "        # make plot including all buildings\n",
    "        fig_ax_0   = fig.add_subplot(gridspec[:, 0:-6])\n",
    "        fig_ax_0_a = fig.add_subplot(gridspec[5:11, -6])\n",
    "        fig_ax_1   = fig.add_subplot(gridspec[:, -3])\n",
    "        \n",
    "        fig_ax_2 = {}\n",
    "        \n",
    "        if normalize_by == 'none':\n",
    "            form = '.0f'\n",
    "            vmax = to_plot.max().max()\n",
    "        elif normalize_by == 'row':\n",
    "            form = '.1%'\n",
    "            vmax = to_plot.max().max()\n",
    "        else:\n",
    "            form = '.1%'\n",
    "            vmax = to_plot.max().max()\n",
    "        \n",
    "        \n",
    "        sns.heatmap(to_plot, square=True, ax=fig_ax_0, annot=True, cbar=True, cbar_ax=fig_ax_0_a, vmin=0, vmax=vmax, fmt=form, cbar_kws={\"shrink\": 0.1}, mask=to_plot.isnull()) \n",
    "        sns.heatmap(dominant_cluster, square=True, ax=fig_ax_1, annot=True, cbar=False, cmap=ListedColormap(['white']), linewidths=.5, linecolor='black')\n",
    "\n",
    "        sparkline_data = self.hourly_data.copy()\n",
    "        sparkline_data['cluster_group'] = self.cluster_data[f'{self.n_clusters+2}_clusters'].copy()\n",
    "\n",
    "        self.maxval_1 = self.get_maxval_1(sparkline_data)\n",
    "\n",
    "        sparkline_data_to_plot = sparkline_data.groupby('cluster_group').mean().T\n",
    "\n",
    "        for i, cluster in enumerate(dominant_cluster.values):\n",
    "            if pd.isna(cluster):\n",
    "                continue\n",
    "            fig_ax_2[i] = fig.add_subplot(gridspec[i, -2:])\n",
    "            \n",
    "            sparkline_data_to_plot[cluster].plot(ax=fig_ax_2[i], legend=False)\n",
    "            \n",
    "            fig_ax_2[i].axes.get_xaxis().set_visible(False)\n",
    "            fig_ax_2[i].axes.get_yaxis().set_visible(False)\n",
    "            fig_ax_2[i].axes.set_ylim([0, self.maxval_1])\n",
    "            fig_ax_2[i].set_aspect(10)\n",
    "            \n",
    "        fig_ax_2[i].axes.get_xaxis().set_visible(True)\n",
    "        fig_ax_2[i].set_xlabel('dominant_cluster_plot')\n",
    "        fig_ax_2[i].set_xticklabels([])\n",
    "        fig_ax_2[i].set_xticks([])\n",
    "            \n",
    "        plt.suptitle(title)\n",
    "        plt.savefig(f'{save_as}_a.png')\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        fig, axes = plt.subplots(ncols=14, nrows=2, figsize=(24, 3))\n",
    "        j = 0\n",
    "        k = 0\n",
    "        for i, cluster in enumerate(sparkline_data_to_plot.columns):\n",
    "            ax = axes[j,k]\n",
    "            \n",
    "            sparkline_data_to_plot[cluster].plot(ax=ax, legend=False)\n",
    "            \n",
    "            ax.axes.set_ylim([0, self.maxval_1])\n",
    "            ax.set_xlim([0,23])\n",
    "    \n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_xticks([5,11,17])\n",
    "            ax.set_yticklabels([])\n",
    "            if self.maxval_1>1:\n",
    "                ax.set_yticks([0, 0.5, 1, 1.5])\n",
    "            else:\n",
    "                ax.set_yticks([0, 0.5, 1])\n",
    "            \n",
    "            ax.set_aspect(10)\n",
    "            \n",
    "            ax.set_title(f'c_{cluster}')\n",
    "            \n",
    "            if j==1:\n",
    "                axes[j,k].set_xticklabels([5,11,17])\n",
    "                \n",
    "            \n",
    "            if k>12:\n",
    "                k=0\n",
    "                j=1\n",
    "            else:\n",
    "                k+=1\n",
    "        \n",
    "        if self.maxval_1>1:\n",
    "            axes[0,0].set_yticklabels([0, 0.5, 1, 1.5])\n",
    "            axes[1,0].set_yticklabels([0, 0.5, 1, 1.5])\n",
    "        else:\n",
    "            axes[0,0].set_yticklabels([0, 0.5, 1])\n",
    "            axes[1,0].set_yticklabels([0, 0.5, 1])\n",
    "            \n",
    "        \n",
    "        \n",
    "        plt.suptitle(f'{self.subfolder} cluster overview')\n",
    "        plt.savefig(f'{save_as}_b.png')\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        # finally, combine plot A and plot B\n",
    "        \n",
    "        list_im = [f'{save_as}_a.png', f'{save_as}_b.png']\n",
    "        imgs    = [ PIL.Image.open(i) for i in list_im ]\n",
    "        \n",
    "        get_concat_v(imgs[0], imgs[1]).save(f'{save_as}.png', dpi=(150, 150))\n",
    "        \n",
    "        for im in list_im:\n",
    "            os.remove(im)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92950d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cluster analysis results\n",
    "sns.set()\n",
    "for folder in type_folders:\n",
    "    subfolders = os.listdir(f'{start_folder_cluster}/{folder}')\n",
    "    for subfolder in subfolders:\n",
    "        print(subfolder)\n",
    "        if subfolder == '.DS_Store':\n",
    "            continue\n",
    "\n",
    "        cluster_plot = ClusterGraphs(start_folder_cluster, folder, subfolder)\n",
    "        \n",
    "        # cluster_plot.plot_graph_1(True)\n",
    "        \n",
    "        # cluster_plot.plot_graph_2(True, False)\n",
    "        \n",
    "        # cluster_plot.plot_graph_2(False, False)\n",
    "\n",
    "        # cluster_plot.plot_graph_3(False)\n",
    "        \n",
    "        cluster_plot.plot_graph_4(True, 'kl')\n",
    "        # cluster_plot.plot_graph_4(True, 'js')\n",
    "        \n",
    "        # cluster_plot.plot_graph_5(False)\n",
    "        # cluster_plot.plot_graph_5(False, 0.90)\n",
    "        \n",
    "        # cluster_plot.plot_graph_6(True, 'row')\n",
    "        # cluster_plot.plot_graph_6(True, 'all')\n",
    "        # cluster_plot.plot_graph_6(True, 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c56064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDomainGraphs:\n",
    "    \n",
    "    def __init__(self, start_folder, folder, file):\n",
    "        self.start_folder = start_folder\n",
    "        self.folder       = folder\n",
    "        self.file         = file\n",
    "        \n",
    "        self.time_result = pd.read_csv(f'{self.start_folder}/{self.folder}/{self.file}', parse_dates=['date'])\n",
    "        self.time_result = self.time_result[self.time_result['bad_data']==False]\n",
    "        self.time_result = self.time_result.rename(columns = {'data_type':'utility'})\n",
    "        self.time_result['date_no_time'] = pd.to_datetime(self.time_result['date'], utc=True, format='%Y-%m-%d').dt.date\n",
    "        \n",
    "        # merge time domain results with weather files\n",
    "        self.time_result = pd.merge(self.time_result, all_weather_data, on = ['date_no_time', 'utility'], how='left').drop('date_no_time', axis=1)\n",
    "        \n",
    "#         self.stats_cols = ['highLoad', 'highLoad_SD', 'baseLoad', 'baseLoad_SD', 'Morning Rise Start',\n",
    "#                            'High Load Start', 'High Load Finish', 'Afternoon Fall Finish',\n",
    "#                            'highLoadDuration', 'riseTime', 'fallTime', 'Base To Peak Ratio']\n",
    "\n",
    "        self.stats_cols = ['highLoad', 'baseLoad', 'Base To Peak Ratio',\n",
    "                           'High Load Start', 'High Load Finish', 'highLoadDuration', \n",
    "                           'Morning Rise Start', 'Afternoon Fall Finish',\n",
    "                           'riseTime', 'fallTime']\n",
    "    \n",
    "    def plot_graph_1(self, show, binsize):\n",
    "        \n",
    "        fig_0, axes_0 = plt.subplots(nrows=10, ncols=3, figsize=(30, 60))\n",
    "        fig_1, axes_1 = plt.subplots(nrows=10, ncols=3, figsize=(30, 60))\n",
    "        \n",
    "        x_cols_0 = {} #{0:'temp_air_rounded', 1:'temp_air_prev_day_rounded', 2:'temp_air_3_day_avg_rounded'}\n",
    "        x_cols_1 = {} #{0:'temp_air_scatter', 1:'temp_air_prev_day_scatter', 2:'temp_air_3_day_avg_scatter'}\n",
    "        x_labels = {} #{0:'air_temp',         1:'previous day air_temp',     2:'3-day average temp'}\n",
    "            \n",
    "        \n",
    "        for i in range(0, 30):\n",
    "            if i/10.0<1:\n",
    "                x_cols_0[i] = 'temp_air_rounded'\n",
    "                x_cols_1[i] = 'temp_air_scatter'\n",
    "                x_labels[i] = 'air_temp'\n",
    "            elif i/10.0 <2:\n",
    "                x_cols_0[i] = 'temp_air_prev_day_rounded'\n",
    "                x_cols_1[i] = 'temp_air_prev_day_scatter'\n",
    "                x_labels[i] = 'previous day air_temp'\n",
    "            else:\n",
    "                x_cols_0[i] = 'temp_air_3_day_avg_rounded'\n",
    "                x_cols_1[i] = 'temp_air_3_day_avg_scatter'\n",
    "                x_labels[i] = '3-day average temp'\n",
    "        \n",
    "        \n",
    "        for i, (ax_0, ax_1) in enumerate(zip(axes_0.T.reshape(-1), axes_1.T.reshape(-1))):\n",
    "            \n",
    "            x_col_0 = x_cols_0[i]\n",
    "            x_col_1 = x_cols_1[i]\n",
    "            \n",
    "            y_col = self.stats_cols[i%10]\n",
    "            \n",
    "            data_to_plot = self.time_result[[y_col, 'temp_air']].copy()\n",
    "            \n",
    "            data_to_plot[['temp_air_rounded']]           = data_to_plot[['temp_air']].apply(c_to_f).apply(lambda x: binsize * round(x/binsize))\n",
    "            data_to_plot[['temp_air_prev_day_rounded']]  = data_to_plot[['temp_air']].shift().apply(c_to_f).apply(lambda x: binsize * round(x/binsize))\n",
    "            data_to_plot[['temp_air_3_day_avg_rounded']] = data_to_plot[['temp_air']].rolling(3, min_periods=3).mean().apply(c_to_f).apply(lambda x: binsize * round(x/binsize))\n",
    "            \n",
    "            data_to_plot[['temp_air_scatter']]           = data_to_plot[['temp_air']].apply(c_to_f)\n",
    "            data_to_plot[['temp_air_prev_day_scatter']]  = data_to_plot[['temp_air']].shift().apply(c_to_f)\n",
    "            data_to_plot[['temp_air_3_day_avg_scatter']] = data_to_plot[['temp_air']].rolling(3, min_periods=3).mean().apply(c_to_f)\n",
    "            \n",
    "            sns.boxplot(data = data_to_plot, x = x_col_0, y=y_col, showfliers=False, color='b', ax = ax_0)\n",
    "            sns.scatterplot(data = data_to_plot, x = x_col_1, y=y_col, color='b', ax = ax_1, alpha=0.05)\n",
    "            \n",
    "            \n",
    "            ax_0.set_title(f'{y_col} vs {x_labels[i]}')\n",
    "            ax_0.set_xlabel(x_labels[i%3])\n",
    "            ax_0.set_ylabel(y_col)\n",
    "            \n",
    "            ax_1.set_title(f'{y_col} vs {x_labels[i]}')\n",
    "            ax_1.set_xlabel(x_labels[i])\n",
    "            ax_1.set_ylabel(y_col)\n",
    "            \n",
    "            labels = ax_0.xaxis.get_ticklabels()\n",
    "            for j, label in enumerate(labels):\n",
    "                if j%2==1:\n",
    "                    label.set_visible(False)\n",
    "        \n",
    "        # make all rows in the plot have the same ylims\n",
    "        maxval_0 = 0\n",
    "        maxval_1 = 0\n",
    "        for i, (ax_0, ax_1) in enumerate(zip(axes_0.reshape(-1), axes_1.reshape(-1))):\n",
    "            maxval_0 = max(ax_0.get_ylim()[1], maxval_0)\n",
    "            maxval_1 = max(ax_1.get_ylim()[1], maxval_1)\n",
    "            \n",
    "            if i%3 == 2:\n",
    "                for (ax_0_0, ax_1_0) in zip(axes_0[int(i/3),:], axes_1[int(i/3),:]):\n",
    "                    ax_0_0.set_ylim([-0.1, maxval_0])\n",
    "                    ax_1_0.set_ylim([-0.1, maxval_1])\n",
    "        \n",
    "                maxval_0 = 0\n",
    "                maxval_1 = 0\n",
    "            \n",
    "        \n",
    "        data_type = self.file.split('.')[0]\n",
    "        data_type = data_type.split('-')[1:]\n",
    "        \n",
    "        if len(data_type)==2:\n",
    "            data_type = f'{data_type[0]}-{data_type[1]}'\n",
    "        else:\n",
    "            data_type = data_type[0]\n",
    "            \n",
    "        \n",
    "        plt.suptitle(f'{data_type} Stats vs Air Temp')\n",
    "        \n",
    "        if not os.path.exists(f'{self.start_folder}/{self.folder}/time_domain_plots'):\n",
    "            os.makedirs(f'{self.start_folder}/{self.folder}/time_domain_plots')\n",
    "        \n",
    "        fig_0.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        fig_1.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        fig_0.savefig(f'{self.start_folder}/{self.folder}/time_domain_plots/{data_type}-boxplot.png',bbox_inches=\"tight\")\n",
    "        fig_1.savefig(f'{self.start_folder}/{self.folder}/time_domain_plots/{data_type}-scatter.png',bbox_inches=\"tight\")\n",
    "        \n",
    "        if show:\n",
    "            fig_0.show()\n",
    "            fig_1.show()\n",
    "        else:\n",
    "            plt.close('all')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aefb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time domain analysis results\n",
    "sns.set()\n",
    "\n",
    "for folder in type_folders:\n",
    "    files = os.listdir(f'{start_folder_time_domain}/{folder}')\n",
    "    for file in files:  \n",
    "        \n",
    "        if 'all_statistics_cluster' not in file:\n",
    "            continue\n",
    "            \n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "        \n",
    "        print(file)\n",
    "\n",
    "        #time_plot = TimeDomainGraphs(start_folder_time_domain, folder, file)\n",
    "\n",
    "        #time_plot.plot_graph_1(False, 5)\n",
    "\n",
    "        del time_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04953b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candle_scatter_plot(data, file, show, binsize, cs_ami):\n",
    "        \n",
    "    n_utilities = len(data['utility'].unique())\n",
    "\n",
    "    if n_utilities <= 1:\n",
    "        fig,axes = plt.subplots(nrows=2, ncols = 1, figsize=(8,12))\n",
    "        ax_0 = axes[0]\n",
    "    else:\n",
    "        fig,axes = plt.subplots(nrows=2, ncols = n_utilities+1, figsize=(8*(n_utilities+1),12))\n",
    "        ax_0 = axes[0,0]\n",
    "\n",
    "    data_to_plot = data[['mean_by_sqft','temp_air']].copy()\n",
    "    data_to_plot['temp_air_rounded'] = myround(data_to_plot['temp_air'], binsize)\n",
    "\n",
    "    x_min = int(data_to_plot['temp_air_rounded'].min())\n",
    "    x_max = int(data_to_plot['temp_air_rounded'].max())+5\n",
    "\n",
    "    color_map = {}\n",
    "    palette = itertools.cycle(sns.color_palette())\n",
    "    next(palette)\n",
    "\n",
    "    building_type = file.split('.')\n",
    "    building_type = building_type[0]\n",
    "\n",
    "    for utility in ['tallahassee', 'seattle', 'epb', 'veic', 'horry', 'cherryland', 'maine', 'fortcollins', 'pge']:\n",
    "        color_map[utility] = next(palette)\n",
    "\n",
    "    to_boxplot = []\n",
    "    for temp in sorted(list(data_to_plot['temp_air_rounded'].unique())):\n",
    "        data_to_plot_tmp = data_to_plot[data_to_plot['temp_air_rounded']==temp]\n",
    "        data_to_plot_tmp = data_to_plot_tmp[~data_to_plot_tmp['mean_by_sqft'].isna()]\n",
    "        to_boxplot.append(list(data_to_plot_tmp['mean_by_sqft']))\n",
    "\n",
    "    if n_utilities <= 1:\n",
    "        utility = list(data['utility'].unique())[0]   \n",
    "        ax_0.boxplot(to_boxplot, positions = sorted(list(data_to_plot['temp_air_rounded'].unique())),showfliers=False, widths=4, patch_artist=True, boxprops=dict(facecolor =color_map[utility], color='k'), medianprops=dict(color='w'))\n",
    "        sns.scatterplot(data = data_to_plot, x='temp_air', y='mean_by_sqft', alpha=0.1, color=color_map[utility], ax = axes[1])\n",
    "\n",
    "        ax_0.set_title(f'{building_type}-{utility} kWh/sf vs air temp')\n",
    "\n",
    "    else:  \n",
    "        ax_0.boxplot(to_boxplot, positions = sorted(list(data_to_plot['temp_air_rounded'].unique())),showfliers=False, widths=4, patch_artist=True, boxprops=dict(facecolor ='b', color='k'), medianprops=dict(color='w'))\n",
    "        sns.scatterplot(data = data_to_plot, x='temp_air', y='mean_by_sqft', alpha=0.1, color='b', ax = axes[1,0])\n",
    "        ax_0.set_title(f'All Utilities kWh/sf vs air temp')\n",
    "\n",
    "    y_max = ax_0.get_ylim()[1]\n",
    "    y_min = -0.0002\n",
    "\n",
    "    ax_0.set_ylabel('kWh/sf')\n",
    "    ax_0.set_xlabel('air temp (Â°F)')\n",
    "\n",
    "    if n_utilities>1:\n",
    "        for i, utility in enumerate(data['utility'].unique()):\n",
    "            data_to_plot = data[['mean_by_sqft','temp_air','utility']].copy()\n",
    "            data_to_plot = data_to_plot[data_to_plot['utility']==utility]\n",
    "\n",
    "            axes[0,i+1].locator_params(integer=True)\n",
    "\n",
    "            data_to_plot['temp_air_rounded'] = myround(data_to_plot['temp_air'], binsize)\n",
    "\n",
    "            to_boxplot = []\n",
    "            for temp in sorted(list(data_to_plot['temp_air_rounded'].unique())):\n",
    "                data_to_plot_tmp = data_to_plot[data_to_plot['temp_air_rounded']==temp]\n",
    "                data_to_plot_tmp = data_to_plot_tmp[~data_to_plot_tmp['mean_by_sqft'].isna()]\n",
    "                to_boxplot.append(list(data_to_plot_tmp['mean_by_sqft']))\n",
    "\n",
    "            axes[0, i+1].boxplot(to_boxplot, positions = sorted(list(data_to_plot['temp_air_rounded'].unique())),showfliers=False, widths = 4,patch_artist=True, boxprops=dict(facecolor =color_map[utility], color='k'), medianprops=dict(color='w'))\n",
    "\n",
    "            sns.scatterplot(data = data_to_plot, x='temp_air', y='mean_by_sqft', alpha=0.1, color=color_map[utility], ax = axes[1,i+1])\n",
    "\n",
    "            y_min = min(y_min, axes[0, i+1].get_ylim()[0])\n",
    "            y_max = max(y_max, axes[0, i+1].get_ylim()[1])\n",
    "\n",
    "            axes[0, i+1].set_title(f'{utility} kWh/sf vs air temp')\n",
    "            axes[0, i+1].set_ylabel('kWh/sf')\n",
    "            axes[0, i+1].set_xlabel('air temp (Â°F)')\n",
    "            \n",
    "            axes[1, i+1].set_ylabel('kWh/sf')\n",
    "            axes[1, i+1].set_xlabel('air temp (Â°F)')\n",
    "\n",
    "        plt.suptitle(f'{building_type} kWh/sf, n_utilities = {n_utilities}')\n",
    "\n",
    "    if n_utilities > 1:\n",
    "        for ax in axes[0,:]:\n",
    "            ax.set_xlim(xmin = x_min, xmax = x_max)\n",
    "            ax.set_ylim(ymin = y_min, ymax = y_max)\n",
    "            for label in ax.xaxis.get_ticklabels()[::2]:\n",
    "                label.set_visible(False)\n",
    "\n",
    "        for ax in axes[1,:]:\n",
    "            ax.set_xlim(xmin = x_min, xmax = x_max)\n",
    "            ax.set_ylim(ymin = y_min, ymax = y_max)\n",
    "\n",
    "    else:\n",
    "        axes[0].set_xlim(xmin = x_min, xmax = x_max)\n",
    "        axes[0].set_ylim(ymin = y_min, ymax = y_max)\n",
    "\n",
    "        axes[1].set_xlim(xmin = x_min, xmax = x_max)\n",
    "        axes[1].set_ylim(ymin = y_min, ymax = y_max)\n",
    "\n",
    "    plt.savefig(f'/Users/jgonzal2/EULP/result/{cs_ami}_energy_intensity/by_building_type/{building_type}-energy_intensity.png')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQFTGraphs:\n",
    "    def __init__(self, start_folder, folder, file):\n",
    "        self.start_folder = start_folder\n",
    "        self.folder       = folder\n",
    "        self.file         = file\n",
    "\n",
    "        self.sqft_result = pd.read_csv(f'{self.start_folder}/{self.folder}/{self.file}', parse_dates=['timestamp'])\n",
    "        self.sqft_result['timestamp'] = pd.to_datetime(self.sqft_result['timestamp'], utc=True)\n",
    "        \n",
    "        # merge time domain results with weather files\n",
    "        self.sqft_result = pd.merge(self.sqft_result, all_weather_data_hourly, on = ['timestamp', 'utility'], how='left')#.drop('date_no_time', axis=1)\n",
    "        self.sqft_result = self.sqft_result[self.sqft_result['temp_air'].notnull()]\n",
    "        self.sqft_result['temp_air'] = self.sqft_result['temp_air'].apply(c_to_f)\n",
    "        \n",
    "    def plot_graph_1(self, show, binsize):\n",
    "        if self.folder != 'by_building_type':\n",
    "            return\n",
    "        \n",
    "        candle_scatter_plot(self.sqft_result, self.file, show, binsize, 'ami')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sqft analysis results\n",
    "sns.set()\n",
    "\n",
    "for folder in type_folders:\n",
    "    files = os.listdir(f'{start_folder_sqft}/{folder}')\n",
    "    for file in files:  \n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "\n",
    "        print(file)\n",
    "        \n",
    "        if file == 'truck_stop.csv':\n",
    "            continue\n",
    "\n",
    "        sqft_plot = SQFTGraphs(start_folder_sqft, folder, file)\n",
    "\n",
    "        sqft_plot.plot_graph_1(False, 5.0)\n",
    "\n",
    "        del sqft_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f850fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComStockGraphs:\n",
    "    def __init__(self, start_folder,folder, file):\n",
    "        \n",
    "        self.start_folder = start_folder\n",
    "        self.folder       = folder\n",
    "        self.file         = file\n",
    "        \n",
    "        self.cs_result = pd.read_csv(f'{start_folder}/{folder}/{file}')\n",
    "        self.cs_result['timestamp'] = pd.to_datetime(self.cs_result['timestamp'], utc=True)\n",
    "        \n",
    "        self.cs_result = pd.merge(self.cs_result, all_weather_data_hourly, on = ['timestamp', 'utility'], how='left')\n",
    "        self.cs_result = self.cs_result[self.cs_result['temp_air'].notnull()]\n",
    "        self.cs_result['temp_air'] = self.cs_result['temp_air'].apply(c_to_f)\n",
    "\n",
    "        \n",
    "    def plot_graph_1(self, show, binsize):\n",
    "        if self.folder != 'by_building_type':\n",
    "            return\n",
    "        \n",
    "        candle_scatter_plot(self.cs_result, self.file, show, binsize, 'cs')\n",
    "        \n",
    "    def plot_graph_2(self):\n",
    "        print(self.cs_result.columns)\n",
    "        \n",
    "        sns.scatterplot(data=self.cs_result, x='timestamp', y='mean_by_sqft', alpha=0.1)\n",
    "        # plt.scatter()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf579f49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the ComStock sqft analysis results\n",
    "sns.set()\n",
    "\n",
    "for folder in type_folders:\n",
    "    files = os.listdir(f'{start_folder_cs_sqft}/{folder}')\n",
    "    for file in files:  \n",
    "        if '.csv' not in file:\n",
    "            continue\n",
    "\n",
    "        print(file)\n",
    "        \n",
    "        if file == 'truck_stop.csv':\n",
    "            continue\n",
    "\n",
    "        cs_plot = ComStockGraphs(start_folder_cs_sqft, folder, file)\n",
    "\n",
    "        cs_plot.plot_graph_1(False, 5.0)\n",
    "        #cs_plot.plot_graph_2()\n",
    "\n",
    "        del cs_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
