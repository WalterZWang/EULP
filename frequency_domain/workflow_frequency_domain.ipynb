{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains routines to analyze the electric load profiles from PG&E office building\n",
    "#### Brief description of this workflow:\n",
    "1. Pre-processing (skip this step your if you have cleaned data)\n",
    "    - Extract data for a building type (e.g. office building in our case)\n",
    "    - Remove empty and problematic data\n",
    "    - Convert data into dataframes ([example](../data_all/1004541105.csv))\n",
    "    - Get typical building load profiles\n",
    "        - Create annual heatmaps\n",
    "        - Use pre-trained CNN model and k-means clustering to distinguish typical building loads (high-load during daytime) and non-typical loads (e.g., high-load during night time)\n",
    "2. Conduct Frequency-Domain analysis\n",
    "    - Annual analysis\n",
    "        - Create bins to group high, medium, and low frequency features\n",
    "    - Daily analysis\n",
    "        - How"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Pre-processing\n",
    "Previously, we found some load profiles have high-peak during the night time, we want to explore how those load profile look like and whether we should separate them from typical load profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/hlee9/Documents/GitHub/DOE_EULP/EULP\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions\n",
    "# Change directory to the EULP root path, use %cd path_to_EULP\n",
    "import os\n",
    "from lib import data_exploration_utils as ex\n",
    "dir_root = %pwd\n",
    "dir_data = os.path.join(dir_root, \"data_all\")\n",
    "dir_fig = os.path.join(dir_root, \"fig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate heatmaps and time-series line plot for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ts_CSVs = ex.get_all_file_paths(dir_data, 'csv')\n",
    "# Create these paths if not exist\n",
    "dir_heatmaps = os.path.join(dir_fig, \"ts_heatmaps\")\n",
    "dir_lines = os.path.join(dir_fig, \"ts_lines\")\n",
    "\n",
    "# for i, ts_csv in enumerate(v_ts_CSVs):\n",
    "#     sp_id = os.path.basename(ts_csv).split('.')[0]\n",
    "#     df_t = ex.clean_pge_df_ts(ts_csv, 2015).dropna()\n",
    "#     df_t = df_t['Value'].squeeze()\n",
    "#     dir_heatmap = os.path.join(dir_heatmaps, f\"{sp_id}.png\")\n",
    "#     try:\n",
    "#         ex.generate_heatmap(df_t, dir_heatmap)\n",
    "#         ex.generate_ts_html(df_t, sp_id, dir_lines)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply k-means clustering with features generated pre-trained CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get conv base features\n",
    "v_heatmaps = ex.get_all_file_paths(dir_heatmaps, 'png')\n",
    "model_conv_base = ex.model_vgg16_conv_base()\n",
    "v_conv_base_features = [ex.get_conv_base_features(img, model_conv_base).flatten() for img in v_heatmaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering (k=2 because we already know there are two distinct patterns) \n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(v_conv_base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy heatmaps to corresponding cluster folder for comparison\n",
    "import shutil\n",
    "kmeans_labels = kmeans.labels_\n",
    "step_1_out_dir = os.path.join(dir_fig, 'step_1')\n",
    "\n",
    "for i, label in enumerate(kmeans_labels):\n",
    "    cluster_dir = os.path.join(step_1_out_dir, f\"cluster_{label}\")\n",
    "    if not os.path.exists(cluster_dir):\n",
    "        os.mkdir(cluster_dir)\n",
    "    shutil.copy(v_heatmaps[i], cluster_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the typical load profiles\n",
    "import numpy as np\n",
    "v_typical_ts_CSVs = list(np.take(v_ts_CSVs, np.where(kmeans_labels==1))[0]) # Caution: cluster label might be 0 for typical load profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Frequency-domain analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get frequency-domain features at annual window level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
